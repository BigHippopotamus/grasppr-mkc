{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425346ad-13d3-4842-a222-396bf24780b0",
   "metadata": {},
   "source": [
    "<h1>Install the necessary packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61e408-118b-41a2-8b85-a47c3e40c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install cvxpy\n",
    "!pip install mosek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2c4ec-d437-4426-9db9-938650a9b242",
   "metadata": {},
   "source": [
    "<h1>Load the GRASP+PR algorithm</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459550c7-3609-4841-82cc-d073ab35695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cvx\n",
    "from itertools import combinations\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# This function returns the distance matrix for a dataset along with the sum of distances between every pair of points\n",
    "def get_distances(data, ord=2):\n",
    "  distances = distance.cdist(data, data, metric='minkowski', p=ord)\n",
    "  weight = np.sum(distances) / 2\n",
    "  return distances, weight\n",
    "\n",
    "\n",
    "# This function calculates the minimum k-partition weight\n",
    "def kcut(data, classes, k):\n",
    "    data = np.array(data)\n",
    "    classes = np.array(classes)\n",
    "    \n",
    "    weight = 0\n",
    "    for cluster in range(k):\n",
    "        locs = np.where(classes == cluster)[0]\n",
    "        \n",
    "        if len(locs) == 0:\n",
    "            continue\n",
    "            \n",
    "        pairs = np.array(list(combinations(data[locs], 2)))\n",
    "        if len(pairs) == 0:\n",
    "            continue\n",
    "        \n",
    "        differences = pairs[:, 0] - pairs[:, 1]\n",
    "        distances = np.linalg.norm(differences, ord=2, axis=1)\n",
    "        weight += np.sum(distances)\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426a9008-a8cc-4bd1-88ea-f1cfa3b3c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function solves the SDP relaxation of the max k-cut problem given in Frieze and Jerrum's 1997 paper\n",
    "def max_k_cut(distances, k):\n",
    "    n = len(distances)\n",
    "  \n",
    "    Y = cvx.Variable((n, n), PSD=True)\n",
    "    constraints = [Y >= (-1 / (k - 1)), cvx.diag(Y) == 1]\n",
    "    expr = cvx.sum(cvx.multiply(distances, np.ones((n, n)) - Y))\n",
    "    problem = cvx.Problem(cvx.Maximize(expr), constraints)\n",
    "    \n",
    "    problem.solve(solver='MOSEK')\n",
    "\n",
    "    y = Y.value\n",
    "    eigenvalues = np.linalg.eigh(y)[0]\n",
    "\n",
    "    if min(eigenvalues) < 0:\n",
    "        y += abs(min(eigenvalues)) * np.identity(n) * 1.00001  # to fix floating-point imprecision\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# This function performs the fixed-point iteration step described in Felzenszwalb et al.'s 2022 paper\n",
    "def fixed_point(x, k, n):\n",
    "    A = ((1 - k/2) / (k-1)) * np.ones(n)\n",
    "    Y = cvx.Variable((n, n), PSD=True)\n",
    "\n",
    "    constraints = [Y >= (-1 / (k - 1)), cvx.diag(Y) == 1]\n",
    "    expr = cvx.sum(cvx.multiply(x + A, Y))\n",
    "    problem = cvx.Problem(cvx.Maximize(expr), constraints)\n",
    "  \n",
    "    problem.solve(solver='MOSEK')\n",
    "    \n",
    "    y = Y.value\n",
    "    eigenvalues = np.linalg.eigh(y)[0]\n",
    "\n",
    "    if min(eigenvalues) < 0:\n",
    "        y += abs(min(eigenvalues)) * np.identity(n) * 1.00001  # to fix floating-point imprecision\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657ec99b-e585-4a44-9904-cbdf189a5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdp_fp(data, fp_iterations, k, distances=None):\n",
    "  n = len(data)\n",
    "  \n",
    "  if distances is None:\n",
    "    distances = distance.cdist(data, data, metric='minkowski', p=2)\n",
    "  \n",
    "  x = max_k_cut(distances, k)\n",
    "  mat = np.reshape(x, (n, n))\n",
    "\n",
    "  for i in range(fp_iterations):\n",
    "    x = fixed_point(x, k, n)\n",
    "\n",
    "  unused = [i for i in range(n)]\n",
    "  classes = [-1 for i in range(n)]\n",
    "  \n",
    "  for i in range(K):\n",
    "      current = unused[0]\n",
    "  \n",
    "      j = 0\n",
    "      while j < len(unused):\n",
    "          if x[current, unused[j]] > 0:\n",
    "              classes[unused[j]] = i\n",
    "              unused.remove(unused[j])\n",
    "          else:\n",
    "              j += 1\n",
    "\n",
    "  return classes, kcut(data, classes, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96df0d-5c37-4dc8-b844-2cd9c3f61ea9",
   "metadata": {},
   "source": [
    "<h1>Select a dataset to load</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb419a4-004d-4198-a674-cbe0331274ac",
   "metadata": {},
   "source": [
    "<h3>Iris</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06245f2-6173-4208-a16b-7f72bac6a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATASET = 'datasets/iris.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=False)\n",
    "\n",
    "data = df.iloc[:, :4].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 4].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ec1d5-fa0f-45c6-9a69-0a4d57d82644",
   "metadata": {},
   "source": [
    "<h3>Palmer Penguins</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1992a12c-99f0-4df8-bb20-dd0d1a543462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/penguins.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=[0])\n",
    "numerical_data = df.get(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'year'])\n",
    "categorical_data = df.get(['island', 'sex'])\n",
    "\n",
    "num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "numerical_data = num_imputer.fit_transform(numerical_data)\n",
    "\n",
    "cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "categorical_data = cat_imputer.fit_transform(categorical_data)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "categorical_data = encoder.fit_transform(categorical_data).toarray()\n",
    "\n",
    "concatenated_data = np.append(numerical_data, categorical_data, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data = scaler.fit_transform(concatenated_data)\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.get('species').values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70febe07-fc7c-4fa4-b408-32738301ac77",
   "metadata": {},
   "source": [
    "<h3>MNIST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24b21c-b1a5-4328-bc23-dfe711c12326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "size = 15000\n",
    "\n",
    "# The subset of the MNIST dataset used was preprocessed and pickled for convenience\n",
    "# due to its large size\n",
    "\n",
    "with open(f'datasets/pickles/mnist{size}_data.npy', 'rb') as data_pickle:\n",
    "  data = np.load(data_pickle)\n",
    "\n",
    "with open(f'datasets/pickles/mnist{size}_output.npy', 'rb') as output_pickle:\n",
    "  output = np.load(output_pickle)\n",
    "\n",
    "distances, all_weights = get_distances(data)\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bc074-a461-4fd5-987f-f08d840f8678",
   "metadata": {},
   "source": [
    "<h3>Crop Recommendation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb85d31-7198-48f3-b76f-70c9212730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATASET = 'datasets/Crop_recommendation.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=False)\n",
    "\n",
    "data = df.iloc[:, :7].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 7].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4db1ac-8422-4e35-924b-c7bfcfe109e0",
   "metadata": {},
   "source": [
    "<h3>Seeds</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a48968-6397-44fc-84a9-c4b995f500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/seeds_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=None, header=None, sep='\\t')\n",
    "\n",
    "data = df.iloc[:, :7].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 7].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462aa67-b74c-4923-b227-699e1dc7e8b6",
   "metadata": {},
   "source": [
    "<h3>Leaf</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729136d-9a72-4e61-a5ca-66f73165c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/leaf.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=[1], header=None)\n",
    "\n",
    "data = df.iloc[:, 1:].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 0].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44bf1e-e9a2-4735-9bf1-41ac0581d9ce",
   "metadata": {},
   "source": [
    "<h3>Wine</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1111d-4d01-437b-a882-b493a593b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/wine.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=None, header=None)\n",
    "\n",
    "data = df.iloc[:, 1:].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 0].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23769ae-d987-4b6b-badd-944276907d0c",
   "metadata": {},
   "source": [
    "<h3>G-set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1bbb5-d7d3-41bb-8602-0c8c46f4d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the G1, G2, G3, G14, G15, G16, G22, G23, G24, G35, G36, G37, G43, G44, G45, G48, G49, and G50 graphs are included\n",
    "\n",
    "def load_G(num):\n",
    "  global distances, all_weights\n",
    "  with open(f'datasets/Gset/G{num}', 'r') as f:\n",
    "    n, lines = map(int, f.readline().split())\n",
    "  \n",
    "    distances = np.zeros((n, n), dtype=np.int32)\n",
    "    for _ in range(lines):\n",
    "      a, b, weight = map(int, f.readline().split())\n",
    "      a -= 1; b -= 1\n",
    "      distances[a, b] = weight\n",
    "      distances[b, a] = weight\n",
    "\n",
    "  all_weights = np.sum(distances) / 2\n",
    "\n",
    "print('Load G_:')\n",
    "g_val = int(input())\n",
    "load_G(g_val)\n",
    "data = None\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ecfa5-3430-4b76-ba69-d1fe65d82127",
   "metadata": {},
   "source": [
    "<h1>Run the algorithm</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1fea5f-c42f-44d2-905d-a231431263df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution has a max k-cut weight of: 25016.881715538613\n",
      "Solution obtains an adjusted Rand index of: 0.7561944834034595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "fp_iterations = 1\n",
    "\n",
    "solution, min_k_partition_weight = sdp_fp(data, fp_iterations, K, distances=distances)\n",
    "max_k_cut_weight = all_weights - min_k_partition_weight\n",
    "\n",
    "print(f'Solution has a max k-cut weight of: {max_k_cut_weight}')\n",
    "print(f'Solution obtains an adjusted Rand index of: {adjusted_rand_score(solution, output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca806eb8-9666-4e68-904f-5789d68b075d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
