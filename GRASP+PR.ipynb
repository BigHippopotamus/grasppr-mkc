{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425346ad-13d3-4842-a222-396bf24780b0",
   "metadata": {},
   "source": [
    "<h1>Install the necessary packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61e408-118b-41a2-8b85-a47c3e40c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install numpy\n",
    "!pip install munkres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2c4ec-d437-4426-9db9-938650a9b242",
   "metadata": {},
   "source": [
    "<h1>Load the GRASP+PR algorithm</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459550c7-3609-4841-82cc-d073ab35695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from munkres import Munkres\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# This function returns the distance matrix for a dataset along with the sum of distances between every pair of points\n",
    "def get_distances(data, ord=2):\n",
    "  distances = distance.cdist(data, data, metric='minkowski', p=ord)\n",
    "  weight = np.sum(distances) / 2\n",
    "  return distances, weight\n",
    "\n",
    "\n",
    "# This function calculates the minimum k-partition weight\n",
    "def kcut(contributions, classes):\n",
    "  return np.sum(contributions[classes, np.arange(contributions.shape[1])]) / 2\n",
    "\n",
    "\n",
    "# This function updates the contribution matrix when moving a point from one cluster to another\n",
    "def updated_contributions(pos, old_cluster, new_cluster, contributions, distances, k):\n",
    "  contributions[old_cluster] -= distances[pos]\n",
    "  contributions[new_cluster] += distances[pos]\n",
    "  return contributions\n",
    "  \n",
    "\n",
    "# Code taken and modified from\n",
    "# https://stackoverflow.com/questions/55258457/find-mapping-that-translates-one-list-of-clusters-to-another-in-python\n",
    "# This function finds maps a clustering to an equivalent clustering nearest to another specified clustering\n",
    "def translate_labels(master_list, convert_list):\n",
    "  cont_mat = contingency_matrix(master_list, convert_list)\n",
    "  munkres = Munkres()\n",
    "  mapping = munkres.compute(cont_mat.max() - cont_mat)\n",
    "\n",
    "  master_labels = np.unique(master_list)\n",
    "  to_convert = np.unique(convert_list)\n",
    "\n",
    "  map = {}\n",
    "  for master_label, convert_label in mapping:\n",
    "    map[to_convert[convert_label]] = master_labels[master_label]\n",
    "\n",
    "  return map\n",
    "\n",
    "\n",
    "# This function determines how different two clusterings are\n",
    "def set_difference(classes, guiding):\n",
    "  mapping = translate_labels(classes, guiding)\n",
    "  guiding = [mapping[class_name] for class_name in guiding]\n",
    "\n",
    "  diff = [pos for pos, (x, y) in enumerate(zip(classes, guiding)) if x != y]\n",
    "\n",
    "  return len(diff) / len(guiding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11c743e2-b7a7-410f-a630-9bc97a25d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# Code for the greedy build phase of the algorithm\n",
    "def greedy_build(k, distances):\n",
    "  assigned_clusters = [i for i in range(k)]\n",
    "  contributions = distances[:k].copy()\n",
    "\n",
    "  for index in range(k, distances.shape[0]):\n",
    "    add_cluster = np.argmin(contributions[:, index])\n",
    "    assigned_clusters.append(add_cluster)\n",
    "    contributions[add_cluster] += distances[index]\n",
    "\n",
    "  return np.array(assigned_clusters), contributions\n",
    "\n",
    "\n",
    "# Code for the local search phase of the algorithm\n",
    "def local_search(classes, k, contributions, distances):\n",
    "  delta_weight = 0\n",
    "  \n",
    "  while True:\n",
    "    local_best = contributions.argmin(axis=0)\n",
    "\n",
    "    if (local_best == classes).all():\n",
    "      break\n",
    "      \n",
    "    pos = np.argmax((local_best - classes) != 0)\n",
    "      \n",
    "    new_cluster = local_best[pos]\n",
    "    cluster = classes[pos]\n",
    "    classes[pos] = new_cluster\n",
    "\n",
    "    delta_weight += contributions[new_cluster, pos] - contributions[cluster, pos]\n",
    "\n",
    "    contributions = updated_contributions(pos, cluster, new_cluster, contributions, distances, k)\n",
    "\n",
    "  return classes, contributions, delta_weight\n",
    "\n",
    "\n",
    "# Code for the path relinking phase of the algorithm\n",
    "def path_relinking(classes, k, elite_set, contributions, distances):\n",
    "  guiding = random.choice(elite_set).copy()\n",
    "\n",
    "  mapping = translate_labels(classes, guiding)\n",
    "  guiding = [mapping[class_name] for class_name in guiding]\n",
    "\n",
    "  diff = [pos for pos, (x, y) in enumerate(zip(classes, guiding)) if x != y]\n",
    "\n",
    "  weight_delta = 0\n",
    "  best_weight_delta = 0\n",
    "\n",
    "  best_classes = classes.copy()\n",
    "  best_contributions = contributions.copy()\n",
    "  while len(diff) > 1:\n",
    "    movements = [contributions[guiding[index], index] - contributions[classes[index], index] for index in diff]\n",
    "\n",
    "    relink_index = np.argmin(movements)\n",
    "    best = movements[relink_index]\n",
    "\n",
    "    point_pos = diff[relink_index]\n",
    "\n",
    "    cluster = classes[point_pos]\n",
    "    new_cluster = guiding[point_pos]\n",
    "\n",
    "    classes[point_pos] = guiding[point_pos]\n",
    "    weight_delta += best\n",
    "\n",
    "    contributions = updated_contributions(point_pos, cluster, new_cluster, contributions, distances, k)\n",
    "\n",
    "    if weight_delta < best_weight_delta:\n",
    "      best_weight_delta = weight_delta\n",
    "      best_classes = classes.copy()\n",
    "      best_contributions = contributions.copy()\n",
    "\n",
    "    del diff[relink_index]\n",
    "\n",
    "  return best_classes, best_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426a9008-a8cc-4bd1-88ea-f1cfa3b3c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import rand_score\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "DEBUG = False\n",
    "DEBUG_SEED = 0\n",
    "\n",
    "\n",
    "def grasp_pr(data, iterations, k, max_elite, distances=None):\n",
    "  REPLACE_THRESHOLD = 0.01\n",
    "\n",
    "  best_weight = None\n",
    "  best_solution = None\n",
    "  elite_set = []\n",
    "  elite_set_weights = []\n",
    "\n",
    "  if distances is None:\n",
    "    distances = distance.cdist(data, data, metric='minkowski', p=2)\n",
    "\n",
    "  print('|', end='')\n",
    "\n",
    "  checkpoint = 1\n",
    "\n",
    "  if DEBUG:\n",
    "    np.random.seed(DEBUG_SEED)\n",
    "    random.seed(DEBUG_SEED)\n",
    "    \n",
    "  for i in range(iterations):\n",
    "    updated_best = False\n",
    "    \n",
    "    shuffle_order = np.arange(distances.shape[0])\n",
    "    np.random.shuffle(shuffle_order)\n",
    "\n",
    "    shuffled_distances = distances[:, shuffle_order][shuffle_order]\n",
    "\n",
    "    build, contributions = greedy_build(k, shuffled_distances)\n",
    "    \n",
    "    build, contributions, _ = local_search(build, k, contributions, shuffled_distances)\n",
    "    \n",
    "    weight = kcut(contributions, build)\n",
    "\n",
    "    if i == 0:\n",
    "      updated_best = False\n",
    "      best_weight = weight\n",
    "\n",
    "      unshuffle_order = np.zeros_like(shuffle_order)\n",
    "      unshuffle_order[shuffle_order] = np.arange(distances.shape[0])\n",
    "      unshuffled_build = build[unshuffle_order]\n",
    "\n",
    "      best_solution = unshuffled_build\n",
    "      elite_set += [unshuffled_build]\n",
    "      elite_set_weights += [weight]\n",
    "\n",
    "    else:\n",
    "      unshuffle_order = np.zeros_like(shuffle_order)\n",
    "      unshuffle_order[shuffle_order] = np.arange(distances.shape[0])\n",
    "      unshuffled_build = build[unshuffle_order]\n",
    "      unshuffled_contributions = contributions[:, unshuffle_order]\n",
    "\n",
    "      if max_elite > 0:\n",
    "        unshuffled_build, unshuffled_contributions = path_relinking(unshuffled_build, k, elite_set, unshuffled_contributions, distances)\n",
    "  \n",
    "        weight = kcut(unshuffled_contributions, unshuffled_build)\n",
    "  \n",
    "        if len(elite_set) < max_elite:\n",
    "          elite_set += [unshuffled_build]\n",
    "          elite_set_weights += [weight]\n",
    "  \n",
    "        else:\n",
    "          if weight < min(elite_set_weights):\n",
    "            worst = np.argmax(elite_set_weights)\n",
    "            elite_set[worst] = unshuffled_build\n",
    "            elite_set_weights[worst] = weight\n",
    "  \n",
    "          elif weight < max(elite_set_weights):\n",
    "            difference = min([set_difference(unshuffled_build, elite_solution) for elite_solution in elite_set])\n",
    "  \n",
    "            if difference > REPLACE_THRESHOLD:\n",
    "              worst = np.argmax(elite_set_weights)\n",
    "              elite_set[worst] = unshuffled_build\n",
    "              elite_set_weights[worst] = weight\n",
    "      else:\n",
    "        weight = kcut(unshuffled_contributions, unshuffled_build)\n",
    "\n",
    "      if weight < best_weight:\n",
    "        updated_best = True\n",
    "        best_weight = weight\n",
    "\n",
    "        best_solution = unshuffled_build\n",
    "\n",
    "    # Prints the progress of the algorithm\n",
    "    if (i + 1) >= checkpoint * (iterations // 100):\n",
    "      print(checkpoint%10, end='')\n",
    "      checkpoint += 1\n",
    "\n",
    "  print()\n",
    "  return best_solution, best_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96df0d-5c37-4dc8-b844-2cd9c3f61ea9",
   "metadata": {},
   "source": [
    "<h1>Select a dataset to load</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb419a4-004d-4198-a674-cbe0331274ac",
   "metadata": {},
   "source": [
    "<h3>Iris</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06245f2-6173-4208-a16b-7f72bac6a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATASET = 'datasets/iris.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=False)\n",
    "\n",
    "data = df.iloc[:, :4].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 4].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ec1d5-fa0f-45c6-9a69-0a4d57d82644",
   "metadata": {},
   "source": [
    "<h3>Palmer Penguins</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1992a12c-99f0-4df8-bb20-dd0d1a543462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/penguins.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=[0])\n",
    "numerical_data = df.get(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'year'])\n",
    "categorical_data = df.get(['island', 'sex'])\n",
    "\n",
    "num_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "numerical_data = num_imputer.fit_transform(numerical_data)\n",
    "\n",
    "cat_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "categorical_data = cat_imputer.fit_transform(categorical_data)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "categorical_data = encoder.fit_transform(categorical_data).toarray()\n",
    "\n",
    "concatenated_data = np.append(numerical_data, categorical_data, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data = scaler.fit_transform(concatenated_data)\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.get('species').values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70febe07-fc7c-4fa4-b408-32738301ac77",
   "metadata": {},
   "source": [
    "<h3>MNIST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b24b21c-b1a5-4328-bc23-dfe711c12326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "size = 15000\n",
    "\n",
    "# The subset of the MNIST dataset used was preprocessed and pickled for convenience\n",
    "# due to its large size\n",
    "\n",
    "with open(f'datasets/pickles/mnist{size}_data.npy', 'rb') as data_pickle:\n",
    "  data = np.load(data_pickle)\n",
    "\n",
    "with open(f'datasets/pickles/mnist{size}_output.npy', 'rb') as output_pickle:\n",
    "  output = np.load(output_pickle)\n",
    "\n",
    "distances, all_weights = get_distances(data)\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bc074-a461-4fd5-987f-f08d840f8678",
   "metadata": {},
   "source": [
    "<h3>Crop Recommendation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb85d31-7198-48f3-b76f-70c9212730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DATASET = 'datasets/Crop_recommendation.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=False)\n",
    "\n",
    "data = df.iloc[:, :7].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 7].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4db1ac-8422-4e35-924b-c7bfcfe109e0",
   "metadata": {},
   "source": [
    "<h3>Seeds</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a48968-6397-44fc-84a9-c4b995f500ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/seeds_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=None, header=None, sep='\\t')\n",
    "\n",
    "data = df.iloc[:, :7].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 7].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462aa67-b74c-4923-b227-699e1dc7e8b6",
   "metadata": {},
   "source": [
    "<h3>Leaf</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729136d-9a72-4e61-a5ca-66f73165c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/leaf.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=[1], header=None)\n",
    "\n",
    "data = df.iloc[:, 1:].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 0].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44bf1e-e9a2-4735-9bf1-41ac0581d9ce",
   "metadata": {},
   "source": [
    "<h3>Wine</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1111d-4d01-437b-a882-b493a593b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "DATASET = 'datasets/wine.csv'\n",
    "\n",
    "df = pd.read_csv(DATASET, index_col=None, header=None)\n",
    "\n",
    "data = df.iloc[:, 1:].values\n",
    "distances, all_weights = get_distances(data)\n",
    "output = df.iloc[:, 0].values\n",
    "\n",
    "K = len(np.unique(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23769ae-d987-4b6b-badd-944276907d0c",
   "metadata": {},
   "source": [
    "<h3>G-set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1bbb5-d7d3-41bb-8602-0c8c46f4d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the G1, G2, G3, G14, G15, G16, G22, G23, G24, G35, G36, G37, G43, G44, G45, G48, G49, and G50 graphs are included\n",
    "\n",
    "def load_G(num):\n",
    "  global distances, all_weights\n",
    "  with open(f'datasets/Gset/G{num}', 'r') as f:\n",
    "    n, lines = map(int, f.readline().split())\n",
    "  \n",
    "    distances = np.zeros((n, n), dtype=np.int32)\n",
    "    for _ in range(lines):\n",
    "      a, b, weight = map(int, f.readline().split())\n",
    "      a -= 1; b -= 1\n",
    "      distances[a, b] = weight\n",
    "      distances[b, a] = weight\n",
    "\n",
    "  all_weights = np.sum(distances) / 2\n",
    "\n",
    "print('Load G_:')\n",
    "g_val = int(input())\n",
    "load_G(g_val)\n",
    "data = None\n",
    "K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ecfa5-3430-4b76-ba69-d1fe65d82127",
   "metadata": {},
   "source": [
    "<h1>Run the algorithm</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1fea5f-c42f-44d2-905d-a231431263df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\n",
      "Solution has a max k-cut weight of: 193629.9696057929\n",
      "Solution obtains an adjusted Rand index of: 0.5396619663398416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "MAX_ELITE = 10\n",
    "iterations = 100\n",
    "\n",
    "solution, min_k_partition_weight = grasp_pr(data, iterations, K, MAX_ELITE, distances=distances)\n",
    "max_k_cut_weight = all_weights - min_k_partition_weight\n",
    "\n",
    "print(f'Solution has a max k-cut weight of: {max_k_cut_weight}')\n",
    "print(f'Solution obtains an adjusted Rand index of: {adjusted_rand_score(solution, output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca806eb8-9666-4e68-904f-5789d68b075d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
